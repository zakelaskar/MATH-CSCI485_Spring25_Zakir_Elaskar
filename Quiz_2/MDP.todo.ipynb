{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf8a3c62-216c-446d-851d-7249cc67787a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "table {align:left;display:block} \n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "table {align:left;display:block} \n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e27c8c-7f53-4e9f-b4da-7e0343fb6313",
   "metadata": {},
   "source": [
    "### Markov Decision Process (MDP)\n",
    "----\n",
    "\n",
    "**Value Iteration Process with Policy Changes in MDP**\n",
    "\n",
    "We begin with a Markov Decision Process (MDP) where an agent decides whether to invest conservatively (C) or aggressively (A) in a financial portfolio. The objective is to find an optimal policy maximizing long-term rewards.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 1: Defining the MDP Components**\n",
    "\n",
    "**States (S):**\n",
    "\n",
    "- Low Wealth (L)\n",
    "- Medium Wealth (M)\n",
    "- High Wealth (H)\n",
    "\n",
    "**Actions (A):**\n",
    "\n",
    "- Conservative (C)\n",
    "- Aggressive (A)\n",
    "\n",
    "**Transition Probabilities:**\n",
    "\n",
    ">| Current State | Action | Next State Probabilities     |\n",
    "| ------------- | ------ | ---------------------------- |\n",
    "| Low (L)       | C      | 80% Stay in L, 20% Move to M |\n",
    "| Low (L)       | A      | 60% Stay in L, 40% Move to M |\n",
    "| Medium (M)    | C      | 70% Stay in M, 30% Move to H |\n",
    "| Medium (M)    | A      | 50% Stay in M, 50% Move to H |\n",
    "| High (H)      | C      | 90% Stay in H, 10% Drop to M |\n",
    "| High (H)      | A      | 70% Stay in H, 30% Drop to M |\n",
    "\n",
    "**Rewards:**\n",
    "\n",
    "- Low Wealth (L): -1\n",
    "- Medium Wealth (M): 3\n",
    "- High Wealth (H): 5\n",
    "\n",
    "**Discount Factor (γ):** 0.9\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf6becd-538b-4010-b4ed-c2e03c6c2e1a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### **Step 2: Value Iteration Updates**\n",
    "\n",
    "We initialize values: $V_0(L) = 0$, $V_0(M) = 0$, $V_0(H) = 0$.\n",
    "\n",
    "#### **Iteration 1**\n",
    "\n",
    "Using Bellman’s equation:\n",
    "\n",
    ">$\n",
    "V_1(s) = \\max_a \\left[ R(s) + \\gamma \\sum_{s'} P(s' | s, a) V_0(s') \\right]\n",
    "$\n",
    "\n",
    "For **Low Wealth (L):**\n",
    "\n",
    ">$\n",
    "V_1(L) = \\max \\left[ -1 + 0.9(0.8V_0(L) + 0.2V_0(M)), -1 + 0.9(0.6V_0(L) + 0.4V_0(M)) \\right]\n",
    "$\n",
    "\n",
    "For **Medium Wealth (M):**\n",
    "\n",
    ">$\n",
    "V_1(M) = \\max \\left[ 3 + 0.9(0.7V_0(M) + 0.3V_0(H)), 3 + 0.9(0.5V_0(M) + 0.5V_0(H)) \\right]\n",
    "$\n",
    "\n",
    "For **High Wealth (H):**\n",
    "\n",
    ">$\n",
    "V_1(H) = \\max \\left[ 5 + 0.9(0.9V_0(H) + 0.1V_0(M)), 5 + 0.9(0.7V_0(H) + 0.3V_0(M)) \\right]\n",
    "$\n",
    "\n",
    "Since $V_0(L) = V_0(M) = V_0(H) = 0$, the initial values are just the rewards.\n",
    "\n",
    ">$\n",
    "V_1(L) = -1, \\quad V_1(M) = 3, \\quad V_1(H) = 5\n",
    "$\n",
    "\n",
    "#### **Policy Evaluation after Iteration 1**\n",
    "\n",
    "> \\$\n",
    "Q(L, C) = -1 + 0.9(0.8(-1) + 0.2(3)) = -1.18\n",
    "\\$\n",
    "\n",
    "> \\$\n",
    "Q(L, A) = -1 + 0.9(0.6(-1) + 0.4(3)) = -0.46\n",
    "\\$\n",
    "\n",
    "> \\$\n",
    "Q(M, C) = 3 + 0.9(0.7(3) + 0.3(5)) = 6.24\n",
    "\\$\n",
    "\n",
    ">\\$\n",
    "Q(M, A) = 3 + 0.9(0.5(3) + 0.5(5)) = 6.6\n",
    "\\$\n",
    "\n",
    ">\\$\n",
    "Q(H, C) = 5 + 0.9(0.9(5) + 0.1(3)) = 9.32\n",
    "\\$\n",
    "\n",
    ">\\$\n",
    "Q(H, A) = 5 + 0.9(0.7(5) + 0.3(3)) = 8.96\n",
    "\\$ \n",
    "\n",
    "\n",
    "**Policy at Iteration 1:**\n",
    "- L → Aggressive (A)\n",
    "- M → Aggressive (A)\n",
    "- H → Conservative (C)\n",
    "\n",
    "\n",
    "#### **Iteration 2**\n",
    "\n",
    "Updating $V_2(s)$:\n",
    "\n",
    ">$\n",
    "V_2(L) = \\max \\left[ -1 + 0.9(0.8(-1) + 0.2(3)), -1 + 0.9(0.6(-1) + 0.4(3)) \\right]\n",
    "$\n",
    "\n",
    ">$\n",
    "V_2(M) = \\max \\left[ 3 + 0.9(0.7(3) + 0.3(5)), 3 + 0.9(0.5(3) + 0.5(5)) \\right]\n",
    "$\n",
    "\n",
    ">$\n",
    "V_2(H) = \\max \\left[ 5 + 0.9(0.9(5) + 0.1(3)), 5 + 0.9(0.7(5) + 0.3(3)) \\right]\n",
    "$\n",
    "\n",
    "Computing the above:\n",
    "\n",
    ">$\n",
    "V_2(L) = -0.46, \\quad V_2(M) = 6.6, \\quad V_2(H) = 9.32\n",
    "$\n",
    "\n",
    "\n",
    "\n",
    "#### **Policy Evaluation after Iteration 2**\n",
    "\n",
    "Complete the iteration...\n",
    "\n",
    "> \\$\n",
    "Q(L, C) = -1 + 0.9(0.8(-0.46) + 0.2(6.6)) = -0.1432\n",
    "\\$\n",
    "\n",
    "> \\$\n",
    "Q(L, A) = -1 + 0.9(0.6(-0.46) + 0.4(6.6)) = 1.1276\n",
    "\\$\n",
    "\n",
    "> \\$\n",
    "Q(M, C) = 3 + 0.9(0.7(6.6) + 0.3(9.32)) = 9.6744\n",
    "\\$\n",
    "\n",
    ">\\$\n",
    "Q(M, A) = 3 + 0.9(0.5(6.6) + 0.5(9.32)) = 10.164\n",
    "\\$\n",
    "\n",
    ">\\$\n",
    "Q(H, C) = 5 + 0.9(0.9(9.32) + 0.1(6.6)) = 13.1432\n",
    "\\$\n",
    "\n",
    ">\\$\n",
    "Q(H, A) = 5 + 0.9(0.7(9.32) + 0.3(6.6)) = 12.6536\n",
    "\\$ \n",
    "\n",
    "\n",
    "**Policy at Iteration 2:**\n",
    "- L → Aggressive (A)\n",
    "- M → Aggressive (A)\n",
    "- H → Conservative (C)\n",
    "\n",
    "#### **Iteration 3**\n",
    "\n",
    "Updating $V_3(s)$:\n",
    "\n",
    "TODO:\n",
    "\n",
    ">$\n",
    "V_3(L) = \\max \\left[ -1 + 0.9(0.8(-0.46) + 0.2(6.6)), -1 + 0.9(0.6(-0.46) + 0.4(6.6)) \\right]\n",
    "$\n",
    "\n",
    ">$\n",
    "V_3(M) = \\max \\left[ 3 + 0.9(0.7(6.6) + 0.3(9.32)), 3 + 0.9(0.5(6.6) + 0.5(9.32)) \\right]\n",
    "$\n",
    "\n",
    ">$\n",
    "V_3(H) = \\max \\left[ 5 + 0.9(0.9(9.32) + 0.1(6.6)), 5 + 0.9(0.7(9.32) + 0.3(6.6)) \\right]\n",
    "$\n",
    "\n",
    "Computing the above:\n",
    "\n",
    ">$\n",
    "V_3(L) = 1.12, \\quad V_3(M) = 10.16, \\quad V_3(H) = 13.14\n",
    "$\n",
    "\n",
    "\n",
    "\n",
    "#### **Policy Change Analysis**\n",
    "\n",
    "From **Iteration 2 to Iteration 3**, let’s check the action values to determine if the policy changed.\n",
    "\n",
    "For **Low Wealth (L):**\n",
    "\n",
    "> \\$\n",
    "Q(L, C) = -1 + 0.9(0.8(1.12) + 0.2(10.16)) = 1.62\n",
    "\\$\n",
    "\n",
    "> \\$\n",
    "Q(L, A) = -1 + 0.9(0.6(1.12) + 0.4(10.16)) = 3.26\n",
    "\\$\n",
    "\n",
    "For **Medium Wealth (M):**\n",
    "\n",
    "> \\$\n",
    "Q(M, C) = 3 + 0.9(0.7(10.16) + 0.3(13.14)) = 12.94\n",
    "\\$\n",
    "\n",
    ">\\$\n",
    "Q(M, A) = 3 + 0.9(0.5(10.16) + 0.5(13.14)) = 13.48\n",
    "\\$\n",
    "\n",
    "For **High Wealth (H):**\n",
    "\n",
    ">\\$\n",
    "Q(H, C) = 5 + 0.9(0.9(13.14) + 0.1(10.16)) = 16.54\n",
    "\\$\n",
    "\n",
    ">\\$\n",
    "Q(H, A) = 5 + 0.9(0.7(13.14) + 0.3(10.16)) = 16.007\n",
    "\\$ \n",
    "\n",
    "\n",
    "Compare $Q(L, A), Q(L, C)$ and $Q(H, C),  Q(H, A)$, decide the policy updates:\n",
    "\n",
    "\n",
    "1. Since, Q(L,C)<Q(L,A), the optimal action for Low Wealth (L) is Aggressive (A).\n",
    "\n",
    "- Policy Update: Switch to Aggressive (A) if it was previously Conservative.\n",
    "\n",
    "\n",
    "\n",
    "2. Since, Q(H,C)>Q(H,A),  the optimal action for Low Wealth (L) is Conservative (C).\n",
    "\n",
    "- Policy Update: Switch to Conservative (C) if it was previously Aggressive (A).\n",
    "\n",
    "- **Low Wealth (L)** → Aggressive (A)\n",
    "- **Medium Wealth (M)** → Aggressive (A)\n",
    "- **High Wealth (H)** → Conservative(C)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0e5695-a01b-4bcc-909b-582b5e64d068",
   "metadata": {},
   "source": [
    "### Summary: Policy Evolution Over Iterations\n",
    "\n",
    ">| State  | Iteration 1 | Iteration 2 | Iteration 3 |\n",
    "|--------|------------|------------|------------|\n",
    "| Low      | Aggressive (A) | Aggressive (A)| Aggressive (A)|\n",
    "| Medium   | Aggressive (A) | Aggressive (A)| Aggressive (A) |\n",
    "| High     | Conservative (C)| Conservative (C)| Conservative (C)|\n",
    "\n",
    "TODO: Analysis:\n",
    "\n",
    "1. Low Wealth (L)\n",
    "All Iterations: The optimal action remains Aggressive (A).\n",
    "\n",
    "Analysis:\n",
    "In all three iterations, the optimal action for Low Wealth remains Aggressive (A).\n",
    "This suggests that the expected rewards from choosing Aggressive (A) outweigh those of Conservative (C) for this state.\n",
    "The higher probability of moving to Medium Wealth (M) with the Aggressive strategy likely leads to higher long-term rewards.\n",
    "\n",
    "2. Medium Wealth (M)\n",
    "All Iterations: The optimal action remains Aggressive (A).\n",
    "\n",
    "Analysis:\n",
    "For the medium-wealth state, the aggressive action consistently yields a higher value across all iterations. This suggests that taking risks at this level provides a higher long-term reward compared to a conservative approach, likely due to the higher probability of transitioning to the high-wealth state.\n",
    "\n",
    "3. High Wealth (H)\n",
    "All Iterations: The optimal action remains Conservative (C).\n",
    "\n",
    "Analysis:\n",
    "Once in the high-wealth state, the conservative approach is preferred across all iterations. This is because it maximizes stability, reducing the risk of dropping to the medium-wealth state. The aggressive action, on the other hand, has a higher probability of transitioning downward, which reduces the long-term expected reward.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fd8328-fc26-4514-8084-7acb5574600e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee52856-a90c-4cc6-90cd-d020f07d061d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
